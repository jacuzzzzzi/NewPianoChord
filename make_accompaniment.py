import miditok
import miditoolkit
import random
import argparse
import torch
from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments
from datasets import Dataset
import miditok
import miditoolkit


def midi_to_tokens(midi_path, mask_ratio=0.2):
    """
    _summary_

    Args:
        midi_path (str): _description_
        mask_ratio (float, optional): _description_. Defaults to 0.2.

    Returns:
        token: _description_
    """
    tokenizer = miditok.REMI()  # ë‹¤ì–‘í•œ ë³€í™˜ ë°©ì‹ ê°€ëŠ¥ (MIDILike, TSD ë“±)

    # MIDI íŒŒì¼ ë¡œë“œ
    midi_data = miditoolkit.MidiFile(midi_path)

    # MIDI â†’ í† í° ë³€í™˜
    tokens = tokenizer(midi_data)

    # ë§ˆìŠ¤í‚¹í•  í† í° ì„ íƒ
    num_masks = int(len(tokens) * mask_ratio)
    masked_tokens = tokens.copy()
    mask_indices = random.sample(range(len(tokens)), num_masks)

    for idx in mask_indices:
        masked_tokens[idx] = "[MASK]"  # íŠ¹ì • í† í°ì„ ë§ˆìŠ¤í‚¹

    token_str = " ".join(masked_tokens)  # LLaMAê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜

    return token_str, tokens  # (ë§ˆìŠ¤í‚¹ëœ ì…ë ¥, ì›ë³¸ ì •ë‹µ)


def prepare_dataset(midi_files, tokenizer: LlamaTokenizer):
    """_summary_

    Args:
        midi_files (_type_): _description_
        tokenizer (LlamaTokenizer): _description_

    Returns:
        _type_: _description_
    """
    masked_inputs = []
    labels = []

    for midi_path in midi_files:
        masked, original = midi_to_tokens(midi_path)
        masked_inputs.append(masked)
        labels.append(original)

    dataset = Dataset.from_dict({
        "input_ids": [tokenizer(m, return_tensors="pt").input_ids for m in masked_inputs],
        "labels": [tokenizer(l, return_tensors="pt").input_ids for l in labels],
    })

    return dataset


def generate_accompaniment(midi_path):
    """_summary_

    Args:
        midi_path (_type_): _description_

    Returns:
        _type_: _description_
    """
    masked_tokens, _ = midi_to_tokens(midi_path)
    input_ids = tokenizer(masked_tokens, return_tensors="pt").input_ids

    with torch.no_grad():
        output = model.generate(input_ids, max_length=512)

    generated_tokens = tokenizer.decode(output[0])
    return generated_tokens


def tokens_to_midi(tokens, output_path):
    """_summary_

    Args:
        tokens (_type_): _description_
        output_path (_type_): _description_
    """
    tokenizer = miditok.REMI()
    generated_midi = tokenizer.tokens_to_midi(tokens)
    generated_midi.dump(output_path)
    print(f"ğŸµ ë°˜ì£¼ MIDI íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MusicBERT ê¸°ë°˜ ë°˜ì£¼ ìƒì„±ê¸°")

    parser.add_argument("--input_midi", type=str, required=True,
                        help="ì…ë ¥í•  ë©œë¡œë”” MIDI íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output_midi", type=str, default="output.mid",
                        help="ìƒì„±ëœ ë°˜ì£¼ë¥¼ ì €ì¥í•  MIDI íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--model", type=str, default="meta-llama/Llama-2-7b-hf",
                        help="ì‚¬ìš©í•  MusicBERT ëª¨ë¸ (ê¸°ë³¸ê°’: facebook/musicbert)")
    parser.add_argument("--max_length", type=int, default=512,
                        help="ìƒì„±í•  ë°˜ì£¼ì˜ ìµœëŒ€ ê¸¸ì´ (ê¸°ë³¸ê°’: 512 í† í°)")
    args = parser.parse_args()
    # LLaMA ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = LlamaTokenizer.from_pretrained(args.model)
    model = LlamaForCausalLM.from_pretrained(args.model)
    dataset = prepare_dataset(*args.input_midi)

    # Training
    training_args = TrainingArguments(
        output_dir="./outputs",
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        num_train_epochs=3,
        save_steps=1000,
        save_total_limit=2,
        logging_dir="./logs",
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
    )
    trainer.train()
